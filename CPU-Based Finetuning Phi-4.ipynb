{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU-Based Fine-Tuning of Small Language Models (SLMs) with Azure Machine Learning Phi-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.Overview**\n",
    "### **[1.1 Motivations for Small Language Models (SLMs)]**\n",
    "- Efficiency: SLM provides higher computational efficiency, requires less memory and storage resources, and runs faster due to a smaller number of parameters, enabling more efficient use of computational resources.\n",
    "- Cost: SLM is less expensive to train and deploy than larger models, making it affordable to a wider range of organizations and particularly suited to resource-constrained environments such as edge computing.\n",
    "- Customizability: SLMs are more flexible in specialized applications and can be more easily fine-tuned for specific tasks than large models. While large models have shown significant benefits, the potential of smaller models for training with large-scale datasets remains untapped, and SLM demonstrates that smaller models trained on sufficient data can also achieve efficient performance.\n",
    "- Reasoning Efficiency: Smaller models are typically more efficient in the inference phase and are especially suitable for deployment in real-world applications with limited resources. Efficient reasoning not only accelerates response time, but also significantly reduces computational and energy costs.\n",
    "- Research accessibility: SLM is open source and small enough to be easily accessible to a wide range of researchers, especially for teams that do not have enough resources to handle larger models. It provides a low-cost research platform for experimentation and innovation in the field of language modeling.\n",
    "- Architectural and Optimization Advances: SLM incorporates a variety of architectural and performance optimization techniques that significantly improve computational efficiency. These optimizations enable SLM to train quickly on common GPUs with low memory consumption.\n",
    "- Open Source Contributions: The developers of SLM have made significant contributions to the open source community by publicly releasing model checkpoints and code, encouraging other researchers to build on this foundation for further innovation and applications.\n",
    "- End-user applications: Due to its excellent performance and compact model structure, SLM is well suited for end-user applications and may even run on mobile devices, thus providing a lightweight platform for a wide range of applications.\n",
    "- Training data and process: SLM's training process is not only efficient but also repeatable, using a mixture of natural language data and code data, designed to make pre-training more transparent and accessible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[1.2 Phi-4 (Microsoft Research)]**\n",
    "Phi-4 is the latest generation of self-supervised language models developed by Microsoft Research, inheriting the success of phi-3 and optimizing it in several ways. phi-4 performs well in several public benchmarks, and in particular makes significant progress in handling long contexts and multimodal tasks. Its support for long contexts up to 256K further improves the model's reasoning and contextual understanding.\n",
    "\n",
    "- Phi-4-mini is a 4.2B parametric language model (256K and 8K).\n",
    "- Phi-4-small is a 9B parametric language model (256K and 16K).\n",
    "- Phi-4-medium is an 18B parametric language model (256K and 8K).\n",
    "- Phi-4 Vision is a 5B parametric multimodal model that integrates language and vision functions to handle more complex cross-modal tasks.\n",
    "- The release of the Phi-4 family will further advance the performance of large-scale models in real-world applications, especially in the areas of complex dialog generation and image understanding.\n",
    "\n",
    "In this example, we will learn how to use QLoRA to fine-tune phi-4-mini-8k-instruct:QLoRA is an efficient fine-tuning technique that quantizes a pre-trained language model to 4 bits and attaches a small fine-tuned “low-rank adapter” to it. In this example, the choice was made to use a cpu for model training, which, despite the longer training time, offers a more flexible and scalable solution with better cost-effectiveness, broader availability, and ease of use for smaller training tasks and debugging, and in some cases may not require the large computational power of a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.Hands-on lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[2.1 How to use Azure Machine Learning]**\n",
    "At the very beginning, I will briefly describe the Azure Machine Learning Getting Started operation. If you are already familiar with Azure Machine Learning, you can skip this step and go directly to the subsequent model fine-tuning steps. For this experiment, we are working on Azure AI Machine Learning Studio. In this tutorial, I will demonstrate training, registering, and deploying the phi-4 model. This tutorial will help you familiarize yourself with the core concepts of Azure Machine Learning and its most common uses. You will learn how to run a training job on a scalable compute resource, then deploy that job, and finally test the deployment. You will create a training script to handle data preparation, training, and registering the model. After training the model, deploy it as a endpoint and then invoke the endpoint for inference.\n",
    "\n",
    "2.1.1 To use Azure Machine Learning, you need a workspace. If you don't have a workspace, complete Create Getting Started with the required resources to create a workspace\n",
    "\n",
    "2.1.2 Log in to the studio and select your workspace (if not already open).\n",
    "\n",
    "2.1.3 Open or create a notebook in your workspace: If you want to copy and paste code into cells, create a new notebook. Alternatively, open from the Examples section of the Studio and select Clone to add the notebook to your File.\n",
    "\n",
    "2.1.4 Setting up the kernel and opening it in Visual Studio Code (VS Code)\n",
    "\n",
    "2.1.5 In the top bar above the open notebook, create a compute instance (if one does not already exist). If the calculation instance is stopped, select “Start calculation” and wait for it to run. Wait for the compute instance to run. Then make sure that the kernel in the upper right corner is Python 3.10 - SDK v2. If it is not, use the drop-down list to select that kernel.\n",
    "2.1.6 If you see a banner prompting you to authenticate, select “Authenticate”.\n",
    "\n",
    "2.1.7 You can run the notebook here or open it in VS Code to get a full Integrated Development Environment (IDE) with the power of Azure Machine Learning Resources. Select “Open in VS Code” and then choose the Web or Desktop option. When started this way, VS Code attaches to the compute instance, kernel, and workspace file system.\n",
    "\n",
    "2.1.8 Before diving into the code, you need a way to reference the workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized location for all projects created when using Azure Machine Learning to be handled. You will create ml_client for the workspace handle. Then, use ml_client to manage resources and jobs. In the next cell, enter your subscription ID, resource group name, and workspace name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[2.2 Preparation]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 First we use the following code to install the datasets library and install the datasets package through the currently running Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 Let's prepare the dataset. In this example, we use the load_dataset function to load the HuggingFaceH4/ultrachat_200k dataset from the Hugging Face database and specify to load the first 2% of the data in the train_sft section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split='train_sft[:2%]')\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.3 Let's create training and testing examples using a shorter version of our dataset. In order to instruct Tune our model, we need to convert the structured examples into a collection of tasks described by instructions. We define a that takes a sample and returns a string with the format instruction. formatting_function\n",
    "We use train_test_split to split the dataset into an 80% training set and a 20% test set, and save the training set and test set as the files train.jsonl and eval.jsonl, respectively (using the JSON Lines format). These files can be used for subsequent machine learning training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "train_dataset.to_json(f\"data/train.jsonl\")\n",
    "test_dataset = dataset['test']\n",
    "test_dataset.to_json(f\"data/eval.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.4 Let's save this training and test dataset in json format. Now, let's load the Azure ML SDK. this will help us create the necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml import Output\n",
    "from azure.ai.ml.constants import AssetTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.5 Now, let's create the workspace client. The function of the following code is to attempt to connect to an Azure ML workspace via the default authentication method (DefaultAzureCredential). If the automatic connection fails (e.g. the profile is unavailable or the necessary configuration information is missing), you will be prompted to manually enter the Azure Subscription ID, Resource Group, and Workspace Name and manually create a connection with this information. In this way, the code successfully creates an instance of the MLClient client to access the Azure Machine Learning service, regardless of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "workspace_ml_client = None\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    subscription_id= \"Enter your subscription_id\"\n",
    "    resource_group = \"Enter your resource_group\"\n",
    "    workspace= \"Enter your workspace name\"\n",
    "    workspace_ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.6 Here, let's create a custom training environment. Define and create an Azure Machine Learning environment llm-training that is based on a specified Docker image (acft-hf-nlp-cpu:latest) and includes a Conda environment profile. The code then uploads this environment to the Azure ML workspace via the workspace_ml_client client. It updates the environment if it already exists, and creates a new one if it does not. This environment will be used for large-scale language model training tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "env_docker_image = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-cpu:latest\",\n",
    "    conda_file=\"environment/conda.yml\",\n",
    "    name=\"llm-training\",\n",
    "    description=\"Environment created for llm training.\",\n",
    ")\n",
    "workspace_ml_client.environments.create_or_update(env_docker_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.7 Let's take a look at conda.yaml. at first my code couldn't read conda.yml properly, so I chose to create the .yml file on my own and upload it to the Azure AI notebook folder, which subsequently ran fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "    - bitsandbytes==0.43.1\n",
    "    - transformers~=4.41\n",
    "    - peft~=0.11\n",
    "    - accelerate~=0.30\n",
    "    - trl==0.8.6\n",
    "    - einops==0.8.0\n",
    "    - datasets==2.19.1\n",
    "    - wandb==0.17.0\n",
    "    - mlflow==2.13.0\n",
    "    - azureml-mlflow==1.56.0 \n",
    "    - torchvision==0.18.0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[2.3 Training]**\n",
    "Let's take a look at the training script. We will use the approach recently introduced in the paper “QLoRA: Quantization-aware low-rank adapter tuning for language generation” by Tim Dettmers et al. QLoRA is a new technique that reduces the memory footprint of a large language model without sacrificing performance during fine-tuning.The TL; Dr;. QLoRA works by:\n",
    "- Quantize a pre-trained model to 4 bits and freeze it.\n",
    "- Attach small, trainable adapter layers. (Lora)\n",
    "- Fine-tune only the adapter layer while using the frozen quantized model as context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1 The following code implements a complete training process for fine-tuning a large-scale pre-trained language model, Phi-4-mini, which is efficiently fine-tuned using PEFT (Lora Configuration), and supports multi-task training, evaluation, logging, and model saving. A customized training process is implemented with SFTTrainer, and Hugging Face's transformers and datasets libraries are used to process the data and model.\n",
    "\n",
    "**Note** ： We disable fp16 and bf16 for cpu training, use smaller batches in “per_device_eval_batch_size” and “per_device_train_batch_size”, and disable “gradient_checkpointing” on CPU to avoid overhead. avoid overhead; also remove Flash Attention, use CPU-friendly float32, explicitly specify loading to CPU in “device_map”; reduce maximum sequence length, set tokenizer.model_max_length and max_seq_length to smaller values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "\n",
    "import os\n",
    "#import mlflow\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "###################\n",
    "# Hyper-parameters\n",
    "###################\n",
    "training_config = {\n",
    "    \"bf16\": False,\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 5.0e-06,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 20,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./checkpoint_dir\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    }\n",
    "\n",
    "peft_config = {\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": \"all-linear\",\n",
    "    \"modules_to_save\": None,\n",
    "}\n",
    "train_conf = TrainingArguments(**training_config)\n",
    "peft_conf = LoraConfig(**peft_config)\n",
    "\n",
    "###############\n",
    "# Setup logging\n",
    "###############\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log_level = train_conf.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process a small summary\n",
    "logger.warning(\n",
    "    f\"Process rank: {train_conf.local_rank}, device: {train_conf.device}, n_gpu: {train_conf.n_gpu}\"\n",
    "    + f\" distributed training: {bool(train_conf.local_rank != -1)}, 16-bits training: {train_conf.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {train_conf}\")\n",
    "logger.info(f\"PEFT parameters {peft_conf}\")\n",
    "\n",
    "################\n",
    "# Modle Loading\n",
    "################\n",
    "checkpoint_path = \"microsoft/Phi-4-mini-8k-instruct\"\n",
    "# checkpoint_path = \"microsoft/Phi-4-mini-8k-instruct\"\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\",  # loading the model with flash-attenstion support\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 2048\n",
    "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "##################\n",
    "# Data Processing\n",
    "##################\n",
    "def apply_chat_template(\n",
    "    example,\n",
    "    tokenizer,\n",
    "):\n",
    "    messages = example[\"messages\"]\n",
    "    # Add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False)\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    train_dataset = load_dataset('json', data_files=args.train_file, split='train')\n",
    "    test_dataset = load_dataset('json', data_files=args.eval_file, split='train')\n",
    "    column_names = list(train_dataset.features)\n",
    "\n",
    "    processed_train_dataset = train_dataset.map(\n",
    "        apply_chat_template,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        num_proc=10,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Applying chat template to train_sft\",\n",
    "    )\n",
    "\n",
    "    processed_test_dataset = test_dataset.map(\n",
    "        apply_chat_template,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        num_proc=10,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Applying chat template to test_sft\",\n",
    "    )\n",
    "\n",
    "    ###########\n",
    "    # Training\n",
    "    ###########\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=train_conf,\n",
    "        peft_config=peft_conf,\n",
    "        train_dataset=processed_train_dataset,\n",
    "        eval_dataset=processed_test_dataset,\n",
    "        max_seq_length=2048,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True\n",
    "    )\n",
    "    train_result = trainer.train()\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "\n",
    "    #############\n",
    "    # Evaluation\n",
    "    #############\n",
    "    tokenizer.padding_side = 'left'\n",
    "    metrics = trainer.evaluate()\n",
    "    metrics[\"eval_samples\"] = len(processed_test_dataset)\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "\n",
    "    # ############\n",
    "    # # Save model\n",
    "    # ############\n",
    "    os.makedirs(args.model_dir, exist_ok=True)\n",
    "    torch.save(model, os.path.join(args.model_dir, \"model.pt\"))\n",
    "\n",
    "def parse_args():\n",
    "    # setup argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--train-file\", type=str, help=\"Input data for training\")\n",
    "    parser.add_argument(\"--eval-file\", type=str, help=\"Input data for eval\")\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=\"./\", help=\"output directory for model\")\n",
    "    parser.add_argument(\"--epochs\", default=10, type=int, help=\"number of epochs\")\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        default=16,\n",
    "        type=int,\n",
    "        help=\"mini batch size for each gpu/process\",\n",
    "    )\n",
    "    parser.add_argument(\"--learning-rate\", default=0.001, type=float, help=\"learning rate\")\n",
    "    parser.add_argument(\"--momentum\", default=0.9, type=float, help=\"momentum\")\n",
    "    parser.add_argument(\n",
    "        \"--print-freq\",\n",
    "        default=200,\n",
    "        type=int,\n",
    "        help=\"frequency of printing training statistics\",\n",
    "    )\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "    # call main function\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2 Let's create a training compute. We choose to import the AmlCompute library for creating, configuring, and managing compute clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "# If you have a specific compute size to work with change it here. By default we use the 1 x A100 compute from the above list\n",
    "\n",
    "compute_cluster_size = \"Standard_E4ds_v4\"  # 4 核 CPU，32GB RAM\n",
    "compute_cluster = \"phi4\"\n",
    "\n",
    "try:\n",
    "    compute = ml_client.compute.get(compute_cluster)\n",
    "    print(\"The compute cluster already exists! Reusing it for the current run\")\n",
    "except Exception as ex:\n",
    "    print(\n",
    "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {compute_cluster_size}!\"\n",
    "    )\n",
    "    try:\n",
    "        print(\"Attempt #1 - Trying to create a dedicated compute\")\n",
    "        compute = AmlCompute(\n",
    "            name=compute_cluster,\n",
    "            size=compute_cluster_size,\n",
    "            tier=\"Dedicated\",\n",
    "            max_instances=1,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    except Exception as e:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know the name of the “compute_cluster_size” and “compute_cluster” in the current compute instance, you can get them using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Connecting to the Azure ML Workspace\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient.from_config(credential)\n",
    "\n",
    "# Get all calculation instances\n",
    "compute_list = ml_client.compute.list()\n",
    "\n",
    "# Print information about all available compute instances\n",
    "for compute in compute_list:\n",
    "    print(f\"Compute Name: {compute.name}, Size: {compute.size}, Type: {compute.type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tips that may help:\n",
    "\n",
    "- LoRA rankings do not need to be very high. (e.g., r=256) In our experience, 8 or 16 baselines are sufficient.\n",
    "- If the training dataset is small, it is better to set rank=alpha. 2*rank or 4*rank training is usually unstable on small datasets.\n",
    "- When using lora, set the learning rate to small. Learning rates such as 1e-3 or 2e-4 are not recommended. We start with 8e-4 or 5e-5.\n",
    "- You should check if we have enough GPU memory instead of setting a larger batch size. This is because if the context length is as long as 8K, OOM (out of memory) may occur. Batch size can be increased using gradient checkpoints and gradient accumulation.\n",
    "- If you are sensitive to batch size and memory, never stick with Adam, including low-bit Adam. Adam requires extra GPU memory to compute the 1st and 2nd momentum. SGD (Stochastic gradient descent) converges slower but does not take up extra GPU memory.\n",
    "\n",
    "Now, let's use the above training script to call a computation job in the AML computation we just created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.3 Now, let's call a computational job using the above training script in the AML computation we just created. Since we are using a CPU with less computational power and less memory, we choose smaller batches, fewer epochs, and more conservative learning rates and momentum in order to optimize resource usage and reduce the computational burden during training, improve training efficiency and avoid memory overflow or system lag.\n",
    "\n",
    "**Note** : The authors failed to create the training environment in this step, and then used the command #version=“15” when creating the environment, which directly specifies the version number of the environment and runs normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.entities import ResourceConfiguration\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        train_file=Input(type=\"uri_file\", path=\"./data/train.jsonl\"),\n",
    "        eval_file=Input(type=\"uri_file\", path=\"./data/eval.jsonl\"),\n",
    "        epoch=1,\n",
    "        batchsize=2,\n",
    "        lr=0.01,\n",
    "        momentum=0.9,\n",
    "        prtfreq=200,\n",
    "        output = \"./outputs\"\n",
    "    ),\n",
    "    code=\"./src\",  # local path where the code is stored\n",
    "    compute = 'phi-4',\n",
    "    command=\"accelerate launch train.py --train-file ${{inputs.train_file}} --eval-file ${{inputs.eval_file}} --epochs ${{inputs.epoch}} --batch-size ${{inputs.batchsize}} --learning-rate ${{inputs.lr}} --momentum ${{inputs.momentum}} --print-freq ${{inputs.prtfreq}} --model-dir ${{inputs.output}}\",\n",
    "    environment=\"azureml:llm-training:latest\",\n",
    "    distribution={\n",
    "        \"type\": \"PyTorch\",\n",
    "        \"process_count_per_instance\": 1,\n",
    "    },\n",
    ")\n",
    "returned_job  = workspace_ml_client.jobs.create_or_update(job)\n",
    "workspace_ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.4 Let's look at the pipeline output. Gets and prints the outputs of the job returned_job. All the outputs of the job are accessed through the job name (job_name). outputs is an object containing a dictionary or similar structure containing the results of the job, through which we can view and download the files or data output by the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the `trained_model` output is available\n",
    "job_name = returned_job.name\n",
    "print(\"pipeline job outputs: \", workspace_ml_client.jobs.get(job_name).outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[2.4 Endpoint]**\n",
    "\n",
    "2.4.1 After fine-tuning the model, let's register the job in the workspace to create the end node. Register the trained phi-4 model as an MLflow model in the Azure ML model registry for subsequent access, management, and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "run_model = Model(\n",
    "    path=f\"azureml://jobs/{job_name}/outputs/artifacts/paths/outputs/mlflow_model_folder\",\n",
    "    name=\"phi-3-finetuned\",\n",
    "    description=\"Model created from run.\",\n",
    "    type=AssetTypes.MLFLOW_MODEL,\n",
    ")\n",
    "model = workspace_ml_client.models.create_or_update(run_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 Let's create the end node, make sure there is an online endpoint available in Azure ML for the model to perform real-time inference, and that an Azure hosted identity is used for authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    IdentityConfiguration,\n",
    "    ManagedIdentityConfiguration,\n",
    ")\n",
    "\n",
    "# Check if the endpoint already exists in the workspace\n",
    "try:\n",
    "    endpoint = workspace_ml_client.online_endpoints.get(endpoint_name)\n",
    "    print(\"---Endpoint already exists---\")\n",
    "except:\n",
    "    # Create an online endpoint if it doesn't exist\n",
    "\n",
    "    # Define the endpoint\n",
    "    endpoint = ManagedOnlineEndpoint(\n",
    "        name=endpoint_name,\n",
    "        description=f\"Test endpoint for {model.name}\",\n",
    "        identity=IdentityConfiguration(\n",
    "            type=\"user_assigned\",\n",
    "            user_assigned_identities=[ManagedIdentityConfiguration(resource_id=uai_id)],\n",
    "        )\n",
    "        if uai_id != \"\"\n",
    "        else None,\n",
    "    )\n",
    "\n",
    "# Trigger the endpoint creation\n",
    "try:\n",
    "    workspace_ml_client.begin_create_or_update(endpoint).wait()\n",
    "    print(\"\\n---Endpoint created successfully---\\n\")\n",
    "except Exception as err:\n",
    "    raise RuntimeError(\n",
    "        f\"Endpoint creation failed. Detailed Response:\\n{err}\"\n",
    "    ) from err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 After creating the end nodes, we can move on to creating the deployment. Initialize the parameters related to the deployment of the model in Azure Machine Learning, especially the settings for online deployments. The goal is to configure the parameters used in the deployment, such as the deployment name, SKU, request timeout, and environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize deployment parameters\n",
    "\n",
    "deployment_name = \"phi3-deploy\"\n",
    "sku_name = \"Standard_NCs_v3\"\n",
    "\n",
    "REQUEST_TIMEOUT_MS = 90000\n",
    "\n",
    "deployment_env_vars = {\n",
    "    \"SUBSCRIPTION_ID\": subscription_id,\n",
    "    \"RESOURCE_GROUP_NAME\": resource_group,\n",
    "    \"UAI_CLIENT_ID\": uai_client_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.4 For the purpose of reasoning, we will use a different base image that specifies the Docker image to be used for the deployment model, and the configuration contains the routing settings for health checks, readiness checks, and scoring requests. It defines how each type of request will be processed through port 5001 and different paths. With these configurations, Azure ML is able to ensure that containers work at different lifecycle stages, such as startup, ready to process requests, and processing inference requests, when the model is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model, Environment\n",
    "env = Environment(\n",
    "    image='mcr.microsoft.com/azureml/curated/foundation-model-inference:latest',\n",
    "    inference_config={\n",
    "        \"liveness_route\": {\"port\": 5001, \"path\": \"/\"},\n",
    "        \"readiness_route\": {\"port\": 5001, \"path\": \"/\"},\n",
    "        \"scoring_route\": {\"port\": 5001, \"path\": \"/score\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.5 Finally we create and deploy an online deployment in Azure Machine Learning. Configure the parameters required for the deployment, such as the deployment name, model used, compute instance type, environment settings, request timeout, health checks, etc. Finally, the code triggers the creation or update of the online deployment through the begin_create_or_update method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    OnlineRequestSettings,\n",
    "    CodeConfiguration,\n",
    "    ManagedOnlineDeployment,\n",
    "    ProbeSettings,\n",
    "    Environment\n",
    ")\n",
    "\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model.id,\n",
    "    instance_type=sku_name,\n",
    "    instance_count=1,\n",
    "    #code_configuration=code_configuration,\n",
    "    environment = env,\n",
    "    environment_variables=deployment_env_vars,\n",
    "    request_settings=OnlineRequestSettings(request_timeout_ms=REQUEST_TIMEOUT_MS),\n",
    "    liveness_probe=ProbeSettings(\n",
    "        failure_threshold=30,\n",
    "        success_threshold=1,\n",
    "        period=100,\n",
    "        initial_delay=500,\n",
    "    ),\n",
    "    readiness_probe=ProbeSettings(\n",
    "        failure_threshold=30,\n",
    "        success_threshold=1,\n",
    "        period=100,\n",
    "        initial_delay=500,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Trigger the deployment creation\n",
    "try:\n",
    "    workspace_ml_client.begin_create_or_update(deployment).wait()\n",
    "    print(\"\\n---Deployment created successfully---\\n\")\n",
    "except Exception as err:\n",
    "    raise RuntimeError(\n",
    "        f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
    "    ) from err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.6 If you want to delete a terminal node, see the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_ml_client.online_deployments.begin_delete(name = deployment_name, \n",
    "                                                    endpoint_name = endpoint_name)\n",
    "workspace_ml_client._online_endpoints.begin_delete(name = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References:*\n",
    "\n",
    "https://learn.microsoft.com/zh-cn/azure/machine-learning/tutorial-azure-ml-in-a-day?view=azureml-api-2\n",
    "https://techcommunity.microsoft.com/blog/machinelearningblog/finetune-small-language-model-slm-phi-3-using-azure-machine-learning/4130399\n",
    "https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/\n",
    "https://www.philschmid.de/sagemaker-falcon-180b-qlora\n",
    "https://github.com/daekeun-ml/azure-llm-fine-tuning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
